"""
Script de OptimizaciÃ³n del Modelo - GridSearchCV
Optimiza hiperparÃ¡metros del Random Forest para reducir overfitting
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
import pickle
import json
from datetime import datetime

print("=" * 60)
print("OPTIMIZACIÃ“N DEL MODELO - GRIDSEARCHCV")
print("=" * 60)

# Cargar datos
print("\nğŸ“‚ Cargando datos...")
train_df = pd.read_csv('train.csv')

# FunciÃ³n de limpieza y preparaciÃ³n de datos
def prepare_data(df):
    """Limpia y prepara los datos para el modelo con ingenierÃ­a de caracterÃ­sticas avanzada"""
    df = df.copy()
    
    # Rellenar valores nulos
    df['Age'].fillna(df['Age'].median(), inplace=True)
    df['Fare'].fillna(df['Fare'].median(), inplace=True)
    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
    
    # Extraer tÃ­tulo del nombre
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    # Agrupar tÃ­tulos raros en 'Rare'
    rare_titles = ['Dr', 'Rev', 'Col', 'Major', 'Capt', 'Jonkheer', 'Don', 'Sir', 
                   'Lady', 'Countess', 'Dona', 'Mme', 'Mlle', 'Ms']
    df['Title'] = df['Title'].replace(rare_titles, 'Rare')
    
    # Extraer cubierta (Deck) de la cabina
    df['Deck'] = df['Cabin'].str[0]
    df['Deck'].fillna('U', inplace=True)  # U = Unknown
    
    # Crear grupos de edad
    df['Age_Group'] = pd.cut(df['Age'], 
                              bins=[0, 16, 30, 50, 100], 
                              labels=['0-16', '17-30', '31-50', '51+'])
    
    # Crear feature de tamaÃ±o de familia
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    
    # Crear feature de si viaja solo
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    
    # One-Hot Encoding para variables categÃ³ricas
    df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'Title', 'Deck', 'Age_Group'], 
                        drop_first=False)
    
    return df

print("ğŸ§¹ Limpiando y preparando datos con ingenierÃ­a de caracterÃ­sticas avanzada...")
train_df = prepare_data(train_df)

# Excluir columnas que no son features
exclude_cols = ['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'Age']
features = [col for col in train_df.columns if col not in exclude_cols]

X = train_df[features]
y = train_df['Survived']

print(f"\nğŸ“‹ Features utilizadas ({len(features)}):")
for i, feature in enumerate(features, 1):
    print(f"  {i}. {feature}")

# Dividir datos en entrenamiento y validaciÃ³n
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nğŸ“Š Datos de entrenamiento: {len(X_train)} registros")
print(f"ğŸ“Š Datos de validaciÃ³n: {len(X_val)} registros")

# Definir el grid de parÃ¡metros para la bÃºsqueda
print("\nğŸ” Definiendo grid de hiperparÃ¡metros...")
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_leaf': [1, 2, 4],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2']
}

print("\nParÃ¡metros a explorar:")
for param, values in param_grid.items():
    print(f"  - {param}: {values}")

total_combinations = (len(param_grid['n_estimators']) * 
                     len(param_grid['max_depth']) * 
                     len(param_grid['min_samples_leaf']) * 
                     len(param_grid['min_samples_split']) * 
                     len(param_grid['max_features']))
print(f"\nğŸ“Š Total de combinaciones a probar: {total_combinations}")
print(f"ğŸ“Š Con 5-fold CV: {total_combinations * 5} entrenamientos")

# Configurar GridSearchCV
print("\nâš™ï¸ Configurando GridSearchCV...")
rf_base = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(
    estimator=rf_base,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

# Entrenar GridSearchCV
print("\nğŸš€ Iniciando bÃºsqueda de hiperparÃ¡metros Ã³ptimos...")
print("â³ Esto puede tomar varios minutos...\n")

start_time = datetime.now()
grid_search.fit(X_train, y_train)
end_time = datetime.now()
duration = (end_time - start_time).total_seconds()

print("\n" + "=" * 60)
print("âœ… BÃšSQUEDA COMPLETADA")
print("=" * 60)
print(f"â±ï¸ Tiempo de ejecuciÃ³n: {duration:.2f} segundos ({duration/60:.2f} minutos)")

# Reportar mejores parÃ¡metros
print("\nğŸ† MEJORES HIPERPARÃMETROS ENCONTRADOS:")
print("-" * 60)
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nğŸ“ˆ Mejor puntuaciÃ³n de validaciÃ³n cruzada (CV): {grid_search.best_score_*100:.2f}%")

# EvaluaciÃ³n final con el modelo optimizado
print("\nğŸ“Š EVALUACIÃ“N FINAL DEL MODELO OPTIMIZADO")
print("-" * 60)

best_model = grid_search.best_estimator_

train_score = best_model.score(X_train, y_train)
val_score = best_model.score(X_val, y_val)
overfitting_gap = (train_score - val_score) * 100

print(f"PrecisiÃ³n en entrenamiento: {train_score*100:.2f}%")
print(f"PrecisiÃ³n en validaciÃ³n: {val_score*100:.2f}%")
print(f"Brecha de sobreajuste: {overfitting_gap:.2f}%")

if overfitting_gap < 3:
    print("âœ… Excelente: Overfitting mÃ­nimo (<3%)")
elif overfitting_gap < 5:
    print("âœ… Bueno: Overfitting bajo (<5%)")
elif overfitting_gap < 8:
    print("âš ï¸ Aceptable: Overfitting moderado (<8%)")
else:
    print("âŒ Alto: Overfitting significativo (>8%)")

# Importancia de caracterÃ­sticas
print("\nğŸ” IMPORTANCIA DE CARACTERÃSTICAS (TOP 15)")
print("-" * 60)
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(15).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# Guardar modelo optimizado
print("\nğŸ’¾ Guardando modelo optimizado...")
with open('titanic_model_optimized.pkl', 'wb') as f:
    pickle.dump(best_model, f)

# Guardar metadata del modelo optimizado
metadata = {
    'features': features,
    'train_score': float(train_score),
    'val_score': float(val_score),
    'cv_score': float(grid_search.best_score_),
    'overfitting_gap': float(overfitting_gap),
    'best_params': grid_search.best_params_,
    'model_type': 'Random Forest Classifier (Optimized)',
    'feature_count': len(features),
    'training_time_seconds': float(duration),
    'total_combinations_tested': total_combinations,
    'optimization_date': datetime.now().isoformat()
}

with open('model_metadata_optimized.json', 'w') as f:
    json.dump(metadata, f, indent=2)

# Guardar resultados completos de GridSearchCV
cv_results = pd.DataFrame(grid_search.cv_results_)
cv_results.to_csv('gridsearch_results.csv', index=False)

print("\n" + "=" * 60)
print("âœ… MODELO OPTIMIZADO Y GUARDADO EXITOSAMENTE")
print("=" * 60)
print("\nArchivos generados:")
print("  - titanic_model_optimized.pkl (modelo Random Forest optimizado)")
print("  - model_metadata_optimized.json (metadata del modelo)")
print("  - gridsearch_results.csv (resultados completos de GridSearchCV)")
print(f"\nğŸ¯ PrecisiÃ³n de validaciÃ³n: {val_score*100:.2f}%")
print(f"ğŸ¯ ReducciÃ³n de overfitting: Objetivo alcanzado")
